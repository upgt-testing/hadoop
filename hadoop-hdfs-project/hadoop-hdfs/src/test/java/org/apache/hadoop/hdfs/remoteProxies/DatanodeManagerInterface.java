package org.apache.hadoop.hdfs.remoteProxies;

public interface DatanodeManagerInterface {
    void resetLastCachingDirectiveSentTime();
    java.lang.String resolveNetworkLocationWithFallBackToDefaultLocation(DatanodeIDInterface arg0);
    void startSlowPeerCollector();
    void clearPendingQueues();
    int getBlockInvalidateLimit(int arg0);
    void setShouldSendCachingCommands(boolean arg0);
    java.lang.String resolveNetworkLocation(DatanodeIDInterface arg0) throws org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;
    DatanodeStorageReportInterface[] getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType arg0);
    java.lang.String toString();
    int getNumLiveDataNodes();
    void setHeartbeatExpireInterval(long arg0);
    boolean shouldCountVersion(DatanodeDescriptorInterface arg0);
    void refreshNodes(org.apache.hadoop.conf.Configuration arg0) throws java.io.IOException;
    NetworkTopologyInterface getNetworkTopology();
    void addCacheCommands(java.lang.String arg0, DatanodeDescriptorInterface arg1, java.util.List<org.apache.hadoop.hdfs.server.protocol.DatanodeCommand> arg2);
    long getHeartbeatRecheckInterval();
    java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> getDecommissioningNodes();
    boolean isInactive(DatanodeInfoInterface arg0);
    void registerDatanode(DatanodeRegistrationInterface arg0) throws org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException, org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;
    HostConfigManagerInterface getHostConfigManager();
    java.util.function.Consumer<java.util.List<org.apache.hadoop.hdfs.protocol.DatanodeInfoWithStorage>> createSecondaryNodeSorter();
    DatanodeDescriptorInterface getDatanode(java.lang.String arg0);
    void setHeartbeatRecheckInterval(int arg0);
    void setNumStaleNodes(int arg0);
    void startAdminOperationIfNecessary(DatanodeDescriptorInterface arg0);
    void removeDecomNodeFromList(java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> arg0);
    void handleLifeline(DatanodeRegistrationInterface arg0, org.apache.hadoop.hdfs.server.protocol.StorageReport[] arg1, long arg2, long arg3, int arg4, int arg5, VolumeFailureSummaryInterface arg6) throws java.io.IOException;
    long getBlocksPerPostponedMisreplicatedBlocksRescan();
    void sortLocatedBlock(LocatedBlockInterface arg0, java.lang.String arg1, java.util.Comparator<org.apache.hadoop.hdfs.protocol.DatanodeInfo> arg2);
    void setDatanodeDead(DatanodeDescriptorInterface arg0);
    void incrementVersionCount(java.lang.String arg0);
    BlockRecoveryCommandInterface getBlockRecoveryCommand(java.lang.String arg0, DatanodeDescriptorInterface arg1) throws java.io.IOException;
    void markAllDatanodesStale();
    void setNumStaleStorages(int arg0);
    long getStaleInterval();
    DatanodeIDInterface parseDNFromHostsEntry(java.lang.String arg0);
    void initSlowPeerTracker(ConfigurationInterface arg0, TimerInterface arg1, boolean arg2);
    int getBlockInvalidateLimit();
    java.util.List<java.lang.String> getNetworkDependenciesWithDefault(DatanodeInfoInterface arg0);
    void stopSlowPeerCollector();
    java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> getAllSlowDataNodes();
    DatanodeDescriptorInterface getDatanodeDescriptor(java.lang.String arg0);
    long getHeartbeatInterval();
    void removeDatanode(DatanodeDescriptorInterface arg0);
    DatanodeCommandInterface[] handleHeartbeat(DatanodeRegistrationInterface arg0, org.apache.hadoop.hdfs.server.protocol.StorageReport[] arg1, java.lang.String arg2, long arg3, long arg4, int arg5, int arg6, int arg7, VolumeFailureSummaryInterface arg8, SlowPeerReportsInterface arg9, SlowDiskReportsInterface arg10) throws java.io.IOException;
    java.lang.String getSlowPeersReport();
    java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> getEnteringMaintenanceNodes();
    java.lang.String getSlowDisksReport();
    int getBlockInvalidateLimitFromHBInterval();
    void setMaxSlowpeerCollectNodes(int arg0);
    DatanodeDescriptorInterface getDatanodeByHost(java.lang.String arg0);
    HeartbeatManagerInterface getHeartbeatManager();
    boolean getEnableAvoidSlowDataNodesForRead();
    DatanodeAdminManagerInterface getDatanodeAdminManager();
    DatanodeDescriptorInterface getDatanode(DatanodeIDInterface arg0) throws org.apache.hadoop.hdfs.protocol.UnregisteredNodeException;
    void sortLocatedStripedBlock(LocatedBlockInterface arg0, java.util.Comparator<org.apache.hadoop.hdfs.protocol.DatanodeInfo> arg1);
    java.util.Set<java.lang.String> getSlowNodesUuidSet();
    long getStaleIntervalFromConf(ConfigurationInterface arg0, long arg1);
    void countSoftwareVersions();
    void sortLocatedBlocks(java.lang.String arg0, java.util.List<org.apache.hadoop.hdfs.protocol.LocatedBlock> arg1);
    void decrementVersionCount(java.lang.String arg0);
    org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats getFSClusterStats();
    void setBalancerBandwidth(long arg0) throws java.io.IOException;
    int getNumStaleNodes();
    java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> getDnDescriptorsFromIpAddr(java.util.List<java.lang.String> arg0);
    SlowDiskTrackerInterface getSlowDiskTracker();
    int getNumOfDataNodes();
    int getMaxSlowpeerCollectNodes();
    java.util.HashMap<java.lang.String, java.lang.Integer> getDatanodesSoftwareVersions();
    void wipeDatanode(DatanodeIDInterface arg0);
    void setBlockInvalidateLimit(int arg0);
    DatanodeCommandInterface getCacheCommand(CachedBlocksListInterface arg0, int arg1, java.lang.String arg2);
    org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStatistics getDatanodeStatistics();
    int getNumStaleStorages();
    void removeDeadDatanode(DatanodeIDInterface arg0, boolean arg1);
    DatanodeDescriptorInterface getDatanodeByXferAddr(java.lang.String arg0, int arg1);
    void removeDatanode(DatanodeDescriptorInterface arg0, boolean arg1);
    void refreshDatanodes();
    org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats newFSClusterStats();
    void datanodeDump(java.io.PrintWriter arg0);
    SlowPeerTrackerInterface getSlowPeerTracker();
    java.util.Set<java.lang.String> getSlowPeersUuidSet();
    void fetchDatanodes(java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> arg0, java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> arg1, boolean arg2);
    void setHeartbeatInterval(long arg0, int arg1);
    void addDatanode(DatanodeDescriptorInterface arg0);
    void setHeartbeatInterval(long arg0);
    boolean isNameResolved(java.net.InetAddress arg0);
    //java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> getDatanodeListForReport(org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType arg0);
    java.util.List<DatanodeDescriptorInterface> getDatanodeListForReport(org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType arg0);
    void checkIfClusterIsNowMultiRack(DatanodeDescriptorInterface arg0);
    void clearPendingCachingCommands();
    int getNumDeadDataNodes();
    void refreshHostsReader(ConfigurationInterface arg0) throws java.io.IOException;
    java.util.Set<DatanodeDescriptorInterface> getDatanodes();
    DatanodeStorageInfoInterface[] getDatanodeStorageInfos(DatanodeIDInterface[] arg0, java.lang.String[] arg1, java.lang.String arg2, java.lang.Object... arg3) throws org.apache.hadoop.hdfs.protocol.UnregisteredNodeException;
    Host2NodesMapInterface getHost2DatanodeMap();
    void addSlowPeers(java.lang.String arg0);
    boolean isDatanodeDead(DatanodeDescriptorInterface arg0);
    java.util.List<java.lang.String> getNetworkDependencies(DatanodeInfoInterface arg0) throws org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;
    boolean isSlowNode(java.lang.String arg0);
    void activate(ConfigurationInterface arg0);
    boolean shouldAvoidStaleDataNodesForWrite();
    void close();
    void setAvoidSlowDataNodesForReadEnabled(boolean arg0);
    void removeDatanode(DatanodeIDInterface arg0) throws org.apache.hadoop.hdfs.protocol.UnregisteredNodeException;
    void resolveUpgradeDomain(DatanodeDescriptorInterface arg0);
    java.util.List<java.lang.String> resolveNetworkLocation(java.util.List<java.lang.String> arg0);
}