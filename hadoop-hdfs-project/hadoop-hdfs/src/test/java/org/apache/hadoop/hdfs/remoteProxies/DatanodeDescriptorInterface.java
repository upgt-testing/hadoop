package org.apache.hadoop.hdfs.remoteProxies;

public interface DatanodeDescriptorInterface {
    void setXceiverCount(int arg0);
    void setNumBlocks(int arg0);
    boolean isDisallowed();
    java.lang.String dumpDatanode();
    void stopMaintenance();
    int compareTo(DatanodeIDInterface arg0);
    void setDisallowed(boolean arg0);
    void setCacheUsed(long arg0);
    java.util.List<java.lang.String> getDependentHostNames();
    void updateHeartbeat(StorageReportInterface[] arg0, long arg1, long arg2, int arg3, int arg4, VolumeFailureSummaryInterface arg5);
    void addBlockToBeReplicated(BlockInterface arg0, DatanodeStorageInfoInterface[] arg1);
    long getCacheCapacity();
    void decrementBlocksScheduled(org.apache.hadoop.fs.StorageType arg0);
    boolean containsInvalidateBlock(BlockInterface arg0);
    int getBlocksScheduled(org.apache.hadoop.fs.StorageType arg0);
    boolean checkBlockReportReceived();
    void addBlocksToBeInvalidated(java.util.List<org.apache.hadoop.hdfs.protocol.Block> arg0);
    long getLastUpdateMonotonic();
    float getDfsUsedPercent();
    long getCacheUsed();
    boolean maintenanceExpired();
    void setLastBlockReportMonotonic(long arg0);
    boolean isHeartbeatedSinceRegistration();
    boolean isAlive();
    java.util.Iterator<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo> getBlockIterator(int arg0);
    void startMaintenance();
    int numBlocks();
    java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.BlockTargetPair> getReplicationCommand(int arg0);
    void decrementPendingReplicationWithoutTargets();
    boolean isInMaintenance();
    int getXceiverCount();
    boolean hasStaleStorages();
    long getLastCachingDirectiveSentTimeMs();
    java.lang.String getIpcAddr(boolean arg0);
    boolean isEnteringMaintenance();
    java.lang.String getHostName();
    void setInMaintenance();
    java.util.Iterator<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo> getBlockIterator(int arg0, DatanodeStorageInfoInterface[] arg1);
    void setForceRegistration(boolean arg0);
    void startDecommission();
    float getRemainingPercent();
    void updateStorageStats(StorageReportInterface[] arg0, long arg1, long arg2, int arg3, int arg4, VolumeFailureSummaryInterface arg5);
    void setLastUpdate(long arg0);
    java.lang.String getUpgradeDomain();
    void setMaintenanceExpireTimeInMS(long arg0);
    int getBlocksScheduled();
    void stopDecommission();
    org.apache.hadoop.net.Node getParent();
    int getLevel();
    void addBlockToBeErasureCoded(ExtendedBlockInterface arg0, DatanodeDescriptorInterface[] arg1, DatanodeStorageInfoInterface[] arg2, byte[] arg3, ErasureCodingPolicyInterface arg4);
    int getNumBlocks();
    long getRemaining();
    BlockInfoInterface[] getLeaseRecoveryCommand(int arg0);
    int getXferPort();
    boolean isStale(long arg0);
    java.lang.String getIpcAddr();
    ByteStringInterface getIpAddrBytes();
    java.util.EnumSet<org.apache.hadoop.fs.StorageType> getStorageTypes();
    void setLevel(int arg0);
    DatanodeStorageInfoInterface getStorageInfo(java.lang.String arg0);
    void setAlive(boolean arg0);
    void setLastBlockReportTime(long arg0);
    long getBalancerBandwidth();
    void setNonDfsUsed(long arg0);
    void incrementPendingReplicationWithoutTargets();
    java.lang.String getIpAddr();
    org.apache.hadoop.hdfs.protocol.DatanodeInfo.AdminStates getAdminState();
    java.lang.String getSoftwareVersion();
    float getCacheUsedPercent();
    void addDependentHostName(java.lang.String arg0);
    java.lang.String getDatanodeUuid();
    void setBalancerBandwidth(long arg0);
    java.util.Iterator<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo> getBlockIterator();
    int getInfoPort();
    boolean needKeyUpdate();
    void setLastCachingDirectiveSentTimeMs(long arg0);
    void setNetworkLocation(java.lang.String arg0);
    void resetBlocks();
    boolean maintenanceNotExpired(long arg0);
    void setDecommissioned();
    StorageReportInterface[] getStorageReports();
    void addBlockToBeRecovered(BlockInfoInterface arg0);
    void setUpgradeDomain(java.lang.String arg0);
    long getCacheRemaining();
    boolean isRegistered();
    void setDependentHostNames(java.util.List<java.lang.String> arg0);
    java.lang.String checkDatanodeUuid(java.lang.String arg0);
    CachedBlocksListInterface getCached();
    long getDfsUsed();
    void updateRegInfo(DatanodeIDInterface arg0);
    long getBlockPoolUsed();
    CachedBlocksListInterface getPendingCached();
    void setPeerHostName(java.lang.String arg0);
    float getCacheRemainingPercent();
    java.lang.String getPeerHostName();
    void setBlockPoolUsed(long arg0);
    java.lang.String getIpcAddrWithHostname();
    ByteStringInterface getDatanodeUuidBytes();
    void setRemaining(long arg0);
    float getBlockPoolUsedPercent();
    java.lang.String getName();
    java.lang.String getXferAddrWithHostname();
    boolean isDecommissionInProgress();
    java.net.InetSocketAddress getResolvedAddress();
    boolean isDecommissioned();
    java.lang.String getInfoSecureAddr();
    long getLastBlockReportTime();
    void incrementBlocksScheduled(org.apache.hadoop.fs.StorageType arg0);
    boolean hasStorageType(org.apache.hadoop.fs.StorageType arg0);
    int getVolumeFailures();
    boolean isMaintenance();
    void setIpAndXferPort(java.lang.String arg0, ByteStringInterface arg1, int arg2);
    int getNumberOfBlocksToBeErasureCoded();
    boolean isInService();
    java.lang.String getInfoAddr();
    CachedBlocksListInterface getPendingUncached();
    ByteStringInterface getHostNameBytes();
    void rollBlocksScheduled(long arg0);
    void updateFailedStorage(java.util.Set<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo> arg0);
    BlockInterface[] getInvalidateBlocks(int arg0);
    long getMaintenanceExpireTimeInMS();
    int getNumberOfReplicateBlocks();
    void pruneStorageMap(StorageReportInterface[] arg0);
    java.util.List<org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand.BlockECReconstructionInfo> getErasureCodeCommand(int arg0);
    ByteStringInterface getByteString(java.lang.String arg0);
    void setParent(org.apache.hadoop.net.Node arg0);
    VolumeFailureSummaryInterface getVolumeFailureSummary();
    java.lang.String getDatanodeReport();
    void injectStorage(DatanodeStorageInfoInterface arg0);
    DatanodeStorageInfoInterface chooseStorage4Block(org.apache.hadoop.fs.StorageType arg0, long arg1);
    long getLastUpdate();
    java.lang.String getXferAddr(boolean arg0);
    void setNeedKeyUpdate(boolean arg0);
    void setCacheCapacity(long arg0);
    DatanodeStorageInfoInterface updateStorage(DatanodeStorageInterface arg0);
    void updateHeartbeatState(StorageReportInterface[] arg0, long arg1, long arg2, int arg3, int arg4, VolumeFailureSummaryInterface arg5);
    java.lang.String getNetworkLocation();
    int getNumberOfBlocksToBeReplicated();
    void setCapacity(long arg0);
    java.lang.String toString();
    int hashCode();
    LeavingServiceStatusInterface getLeavingServiceStatus();
    int getInfoSecurePort();
    void setAdminState(org.apache.hadoop.hdfs.protocol.DatanodeInfo.AdminStates arg0);
    int getIpcPort();
    long getCapacity();
    void setDfsUsed(long arg0);
    void setSoftwareVersion(java.lang.String arg0);
    java.lang.String getXferAddr();
    DatanodeStorageInfoInterface[] getStorageInfos();
    long getNonDfsUsed();
    void setLastUpdateMonotonic(long arg0);
    void setIpAddr(java.lang.String arg0);
    long getLastBlockReportMonotonic();
    boolean equals(java.lang.Object arg0);
    void clearBlockQueues();
}