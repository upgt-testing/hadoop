package org.apache.hadoop.hdfs.remoteProxies;

public interface BlockInfoStripedInterface {
    java.lang.String getBlockName();
    boolean isCompleteOrCommitted();
    BlockUnderConstructionFeatureInterface getUnderConstructionFeature();
    boolean removeStorage(DatanodeStorageInfoInterface arg0);
    void initIndices();
    int compareTo(BlockInterface arg0);
    int hashCode();
    int getCellSize();
    long getBlockCollectionId();
    void ensureCapacity(int arg0, boolean arg1);
    void setNumBytes(long arg0);
    int findStorageInfo(DatanodeStorageInfoInterface arg0);
    void writeHelper(java.io.DataOutput arg0) throws java.io.IOException;
    long spaceConsumed();
    boolean isDeleted();
    boolean isBlockFilename(java.io.File arg0);
    short getReplication();
    BlockInfoInterface getPrevious(int arg0);
    boolean isComplete();
    java.lang.Iterable<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped.StorageAndBlockIndex> getStorageAndIndexInfos();
    void appendStringTo(java.lang.StringBuilder arg0);
    BlockInfoInterface getNext(int arg0);
    void convertToBlockUnderConstruction(org.apache.hadoop.hdfs.server.common.HdfsServerConstants.BlockUCState arg0, DatanodeStorageInfoInterface[] arg1);
    long filename2id(java.lang.String arg0);
    void set(long arg0, long arg1, long arg2);
    void writeId(java.io.DataOutput arg0) throws java.io.IOException;
    BlockInfoInterface setPrevious(int arg0, BlockInfoInterface arg1);
    long getGenerationStamp(java.lang.String arg0);
    boolean isMetaFilename(java.lang.String arg0);
    void addStorage(DatanodeStorageInfoInterface arg0, int arg1, int arg2);
    void delete();
    void setNext(org.apache.hadoop.util.LightWeightGSet.LinkedElement arg0);
    short getRealDataBlockNum();
    void readId(java.io.DataInput arg0) throws java.io.IOException;
    short getTotalBlockNum();
    void write(java.io.DataOutput arg0) throws java.io.IOException;
    long getNumBytes();
    void setStorageInfo(int arg0, DatanodeStorageInfoInterface arg1);
    void convertToCompleteBlock();
    BlockInfoInterface setNext(int arg0, BlockInfoInterface arg1);
    BlockInterface getBlockOnStorage(DatanodeStorageInfoInterface arg0);
    boolean matchingIdAndGenStamp(BlockInterface arg0, BlockInterface arg1);
    DatanodeStorageInfoInterface getStorageInfo(int arg0);
    int findSlot();
    java.util.Iterator<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo> getStorageInfos();
    long getBlockId(java.lang.String arg0);
    boolean isProvided();
    java.lang.String toString(BlockInterface arg0);
    BlockInfoInterface moveBlockToHead(BlockInfoInterface arg0, DatanodeStorageInfoInterface arg1, int arg2, int arg3);
    int getCapacity();
    long getGenerationStamp();
    boolean equals(java.lang.Object arg0);
    short getDataBlockNum();
    short getParityBlockNum();
    boolean isStriped();
    java.io.File metaToBlockFile(java.io.File arg0);
    short getRealTotalBlockNum();
    java.lang.String toString();
    BlockInfoInterface listRemove(BlockInfoInterface arg0, DatanodeStorageInfoInterface arg1);
    boolean isUnderRecovery();
    void setGenerationStamp(long arg0);
    boolean addStorage(DatanodeStorageInfoInterface arg0, BlockInterface arg1);
    long getBlockId();
    void setBlockCollectionId(long arg0);
    BlockInfoInterface listInsert(BlockInfoInterface arg0, DatanodeStorageInfoInterface arg1);
//    java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction> setGenerationStampAndVerifyReplicas(long arg0);
    org.apache.hadoop.hdfs.server.common.HdfsServerConstants.BlockUCState getBlockUCState();
    boolean hasNoStorage();
    void readHelper(java.io.DataInput arg0) throws java.io.IOException;
    byte getStorageBlockIndex(DatanodeStorageInfoInterface arg0);
    ErasureCodingPolicyInterface getErasureCodingPolicy();
//    java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction> commitBlock(BlockInterface arg0) throws java.io.IOException;
    int numNodes();
    void setReplication(short arg0);
    org.apache.hadoop.util.LightWeightGSet.LinkedElement getNext();
    org.apache.hadoop.hdfs.protocol.BlockType getBlockType();
    DatanodeStorageInfoInterface findStorageInfo(DatanodeDescriptorInterface arg0);
    void readFields(java.io.DataInput arg0) throws java.io.IOException;
    DatanodeDescriptorInterface getDatanode(int arg0);
    int findStorageInfoFromEnd(DatanodeStorageInfoInterface arg0);
    void setBlockId(long arg0);
}