package org.apache.hadoop.hdfs.remoteProxies;

import org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks;

public interface FSNamesystemInterface {
    void setEditLogTailerForTests(EditLogTailerInterface arg0);
    long getBlockDeletionStartTime();
    long getCapacityTotal();
    boolean isInManualOrResourceLowSafeMode();
    ECTopologyVerifierResultInterface getECTopologyResultForPolicies(java.lang.String[] arg0) throws java.io.IOException;
    void startRollingUpgradeInternalForNonHA(long arg0) throws java.io.IOException;
    void appendClientPortToCallerContextIfAbsent();
    boolean checkFileProgress(java.lang.String arg0, INodeFileInterface arg1, boolean arg2);
    long getPendingReconstructionBlocks();
    void checkSuperuserPrivilege(FSPermissionCheckerInterface arg0) throws org.apache.hadoop.security.AccessControlException;
    boolean isClientPortInfoAbsent(CallerContextInterface arg0);
    NamespaceInfoInterface getNamespaceInfo();
    ExtendedBlockInterface getExtendedBlock(BlockInterface arg0);
    void concat(java.lang.String arg0, java.lang.String[] arg1, boolean arg2) throws java.io.IOException;
    void checkAccess(java.lang.String arg0, org.apache.hadoop.fs.permission.FsAction arg1) throws java.io.IOException;
    void checkStoragePolicyEnabled(java.lang.String arg0, boolean arg1) throws java.io.IOException;
    SecretManagerStateInterface saveSecretManagerState();
    void logAuditEvent(boolean arg0, java.lang.String arg1, java.lang.String arg2) throws java.io.IOException;
    void removeLeasesAndINodes(java.util.List<java.lang.Long> arg0, java.util.List<org.apache.hadoop.hdfs.server.namenode.INode> arg1, boolean arg2);
    boolean renameTo(java.lang.String arg0, java.lang.String arg1, boolean arg2) throws java.io.IOException;
    boolean isFileClosed(java.lang.String arg0) throws java.io.IOException;
    java.lang.String getHAState();
    java.lang.String getNameDirSize();
    ECBlockGroupStatsInterface getECBlockGroupStats();
    void startActiveServices() throws java.io.IOException;
    int getNumInMaintenanceDeadDataNodes();
    long getLowRedundancyReplicatedBlocks();
    void setBlockManagerForTesting(BlockManagerInterface arg0);
    void setAcl(java.lang.String arg0, java.util.List<org.apache.hadoop.fs.permission.AclEntry> arg1) throws java.io.IOException;
    void reencryptEncryptionZoneInt(FSPermissionCheckerInterface arg0, java.lang.String arg1, org.apache.hadoop.hdfs.protocol.HdfsConstants.ReencryptAction arg2, boolean arg3) throws java.io.IOException;
    void closeFileCommitBlocks(java.lang.String arg0, INodeFileInterface arg1, BlockInfoInterface arg2) throws java.io.IOException;
    java.lang.String getFSState();
    RollingUpgradeInfoInterface queryRollingUpgrade() throws java.io.IOException;
    DatanodeInfoInterface[] datanodeReport(org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType arg0) throws java.io.IOException;
    boolean isAllowedDelegationTokenOp() throws java.io.IOException;
    void setRollingUpgradeInfo(boolean arg0, long arg1);
    void startSecretManagerIfNecessary();
    long renewDelegationToken(TokenInterface<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier> arg0) throws org.apache.hadoop.security.token.SecretManager.InvalidToken, java.io.IOException;
    void metaSave(java.lang.String arg0) throws java.io.IOException;
    void startSecretManager();
    long getCorruptReplicatedBlocks();
    SnapshotDiffReportInterface getSnapshotDiffReport(java.lang.String arg0, java.lang.String arg1, java.lang.String arg2) throws java.io.IOException;
    void loadSecretManagerStateCompat(java.io.DataInput arg0) throws java.io.IOException;
    void checkSuperuserPrivilege() throws org.apache.hadoop.security.AccessControlException;
    int getNumInServiceLiveDataNodes();
    void closeFile(java.lang.String arg0, INodeFileInterface arg1);
    INodeFileInterface getBlockCollection(BlockInfoInterface arg0);
    long getBytesInFutureReplicatedBlocks();
    void addCacheEntry(byte[] arg0, int arg1);
    void finalizeRollingUpgradeInternal(long arg0);
    void stopStandbyServices() throws java.io.IOException;
    long getTotalBlocks();
    java.util.concurrent.locks.ReentrantLock getCpLockForTests();
    int getEffectiveLayoutVersion();
    java.lang.String getNameJournalStatus();
    org.apache.hadoop.hdfs.protocol.HdfsFileStatus getFileInfo(java.lang.String arg0, boolean arg1, boolean arg2, boolean arg3) throws java.io.IOException;
    long getPreferredBlockSize(java.lang.String arg0) throws java.io.IOException;
    void setXAttr(java.lang.String arg0, XAttrInterface arg1, java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag> arg2, boolean arg3) throws java.io.IOException;
    boolean isExternalInvocation();
    boolean checkBlocksComplete(java.lang.String arg0, boolean arg1, org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo... arg2);
    long addCacheDirective(CacheDirectiveInfoInterface arg0, java.util.EnumSet<org.apache.hadoop.fs.CacheFlag> arg1, boolean arg2) throws java.io.IOException;
    java.lang.String getSnapshotStats();
    void writeLockInterruptibly() throws java.lang.InterruptedException;
    void setNeedRollbackFsImage(boolean arg0);
    boolean isInSafeMode();
    void startStandbyServices(ConfigurationInterface arg0, boolean arg1) throws java.io.IOException;
    long getCorruptReplicaBlocks();
    int getThreads();
    void setImageLoaded();
    long getFree();
    java.lang.String getVersion();
    boolean isRollingUpgrade();
    EncryptionZoneInterface getEZForPath(java.lang.String arg0) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;
    void writeUnlock(java.lang.String arg0, boolean arg1);
    java.lang.String getEnteringMaintenanceNodes();
    void finalizeINodeFileUnderConstruction(java.lang.String arg0, INodeFileInterface arg1, int arg2, boolean arg3) throws java.io.IOException;
    void finalizeUpgrade() throws java.io.IOException;
    void updatePipelineInternal(java.lang.String arg0, ExtendedBlockInterface arg1, ExtendedBlockInterface arg2, DatanodeIDInterface[] arg3, java.lang.String[] arg4, boolean arg5) throws java.io.IOException;
    void sortLocatedBlocks(java.lang.String arg0, LocatedBlocksInterface arg1);
    int getWriteHoldCount();
    boolean isRunning();
    boolean isUpgradeFinalized();
    int getIntCookie(java.lang.String arg0);
    void setImageLoaded(boolean arg0);
    long getNumberOfSnapshottableDirs();
    void removeSnapshottableDirs(java.util.List<org.apache.hadoop.hdfs.server.namenode.INodeDirectory> arg0);
    long getExcessBlocks();
    void startRollingUpgradeInternal(long arg0) throws java.io.IOException;
    int getNumDecomDeadDataNodes();
    DelegationTokenSecretManagerInterface createDelegationTokenSecretManager(ConfigurationInterface arg0);
    long getCurrentTokensCount();
    byte[] getSrcPathsHash(java.lang.String[] arg0);
    void writeUnlock();
    long getCacheCapacity();
    void reencryptEncryptionZone(java.lang.String arg0, org.apache.hadoop.hdfs.protocol.HdfsConstants.ReencryptAction arg1, boolean arg2) throws java.io.IOException;
    void modifyCacheDirective(CacheDirectiveInfoInterface arg0, java.util.EnumSet<org.apache.hadoop.fs.CacheFlag> arg1, boolean arg2) throws java.io.IOException;
    long getPendingReplicationBlocks();
    java.util.Collection<java.net.URI> getStorageDirs(ConfigurationInterface arg0, java.lang.String arg1);
    java.util.List<org.apache.hadoop.hdfs.server.namenode.AuditLogger> initAuditLoggers(ConfigurationInterface arg0);
    java.lang.String getSoftwareVersion();
    long getScheduledReplicationBlocks();
    long getPostponedMisreplicatedBlocks();
    void processIncrementalBlockReport(DatanodeIDInterface arg0, StorageReceivedDeletedBlocksInterface arg1) throws java.io.IOException;
    void processIncrementalBlockReport(DatanodeIDInterface arg0, StorageReceivedDeletedBlocks arg1) throws java.io.IOException;
    java.lang.String getTotalSyncTimes();
    void verifyToken(DelegationTokenIdentifierInterface arg0, byte[] arg1) throws org.apache.hadoop.security.token.SecretManager.InvalidToken, org.apache.hadoop.ipc.RetriableException;
    void checkFsObjectLimit() throws java.io.IOException;
    LastBlockWithStatusInterface appendFile(java.lang.String arg0, java.lang.String arg1, java.lang.String arg2, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag> arg3, boolean arg4) throws java.io.IOException;
    long getTotalECBlockGroups();
    void logReassignLease(java.lang.String arg0, java.lang.String arg1, java.lang.String arg2);
    void setManualAndResourceLowSafeMode(boolean arg0, boolean arg1);
    long getTotal();
    long getCacheUsed();
    boolean shouldUseDelegationTokens();
    float getCapacityRemainingGB();
    void metaSave(java.io.PrintWriter arg0);
    void removeXAttr(java.lang.String arg0, XAttrInterface arg1, boolean arg2) throws java.io.IOException;
    java.lang.Object getLastBlockReport(DatanodeDescriptorInterface arg0);
    java.lang.String getSafeModeTip();
    FSEditLogInterface getEditLog();
    boolean mkdirs(java.lang.String arg0, PermissionStatusInterface arg1, boolean arg2) throws java.io.IOException;
    AclStatusInterface getAclStatus(java.lang.String arg0) throws java.io.IOException;
    int getNumberOfDatanodes(org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType arg0);
    long getEstimatedCapacityLostTotal();
    int getNumDecommissioningDataNodes();
    org.apache.hadoop.ha.HAServiceProtocol.HAServiceState getState();
    long getBytesInFutureECBlockGroups();
    void stopCommonServices();
    BlockStoragePolicyInterface getStoragePolicy(java.lang.String arg0) throws java.io.IOException;
    int getNumEnteringMaintenanceDataNodes();
    java.lang.String createSnapshot(java.lang.String arg0, java.lang.String arg1, boolean arg2) throws java.io.IOException;
    java.lang.String metaSaveAsString();
    KeyProviderInterface getProvider();
    void createEncryptionZone(java.lang.String arg0, java.lang.String arg1, boolean arg2) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.hdfs.server.namenode.SafeModeException, org.apache.hadoop.security.AccessControlException;
    boolean hasRetryCache();
    void setOwner(java.lang.String arg0, java.lang.String arg1, java.lang.String arg2) throws java.io.IOException;
    void logFsckEvent(boolean arg0, java.lang.String arg1, java.net.InetAddress arg2) throws java.io.IOException;
    SnapshottableDirectoryStatusInterface[] getSnapshottableDirListing() throws java.io.IOException;
    java.lang.String getFailedStorageCommand(java.lang.String arg0);
    long getCapacityUsedNonDFS();
    int getEffectiveLayoutVersion(boolean arg0, int arg1, int arg2, int arg3);
    boolean isInStandbyState();
    void cpLockInterruptibly() throws java.lang.InterruptedException;
    java.util.List<java.lang.String> getCorruptFilesList();
    FsServerDefaultsInterface getServerDefaults() throws org.apache.hadoop.ipc.StandbyException;
    org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection getBlockCollection(long arg0);
    long getCapacityUsed();
    void setCreatedRollbackImages(boolean arg0);
    LocatedBlockInterface getAdditionalDatanode(java.lang.String arg0, long arg1, ExtendedBlockInterface arg2, DatanodeInfoInterface[] arg3, java.lang.String[] arg4, java.util.Set<org.apache.hadoop.net.Node> arg5, int arg6, java.lang.String arg7) throws java.io.IOException;
    FSNamesystemInterface loadFromDisk(ConfigurationInterface arg0) throws java.io.IOException;
    boolean isPermissionEnabled();
    boolean isInStartupSafeMode();
    int getNumEncryptionZones();
    BlockInterface createNewBlock(org.apache.hadoop.hdfs.protocol.BlockType arg0) throws java.io.IOException;
    void removeErasureCodingPolicy(java.lang.String arg0, boolean arg1) throws java.io.IOException;
    float getCapacityTotalGB();
    void setStoragePolicy(java.lang.String arg0, java.lang.String arg1) throws java.io.IOException;
    RollingUpgradeInfoInterface startRollingUpgrade() throws java.io.IOException;
    FSPermissionCheckerInterface getPermissionChecker() throws org.apache.hadoop.security.AccessControlException;
    void satisfyStoragePolicy(java.lang.String arg0, boolean arg1) throws java.io.IOException;
    void setErasureCodingPolicy(java.lang.String arg0, java.lang.String arg1, boolean arg2) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.hdfs.server.namenode.SafeModeException, org.apache.hadoop.security.AccessControlException;
    java.lang.String getDeadNodes();
    java.lang.String getEnabledEcPolicies();
    void modifyCachePool(CachePoolInfoInterface arg0, boolean arg1) throws java.io.IOException;
    boolean inTransitionToActive();
    BeanInterface getRollingUpgradeStatus();
    float getPercentUsed();
    long getMissingReplicatedBlocks();
    int getVolumeFailuresTotal();
    DatanodeInfoInterface[] getDatanodeInfoFromDescriptors(java.util.List<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor> arg0);
    BatchedListEntriesInterface<org.apache.hadoop.hdfs.protocol.ZoneReencryptionStatus> listReencryptionStatus(long arg0) throws java.io.IOException;
    void removeAcl(java.lang.String arg0) throws java.io.IOException;
    boolean setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants.SafeModeAction arg0) throws java.io.IOException;
    void enterSafeMode(boolean arg0) throws java.io.IOException;
    void setBlockPoolId(java.lang.String arg0);
    NamenodeCommandInterface startCheckpoint(NamenodeRegistrationInterface arg0, NamenodeRegistrationInterface arg1) throws java.io.IOException;
    void logAuditEvent(boolean arg0, UserGroupInformationInterface arg1, java.net.InetAddress arg2, java.lang.String arg3, java.lang.String arg4, java.lang.String arg5, FileStatusInterface arg6);
    QuotaUsageInterface getQuotaUsage(java.lang.String arg0) throws java.io.IOException;
    void loadSecretManagerState(SecretManagerSectionInterface arg0, java.util.List<org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey> arg1, java.util.List<org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken> arg2, org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress.Counter arg3) throws java.io.IOException;
    org.apache.hadoop.hdfs.protocol.HdfsFileStatus startFile(java.lang.String arg0, PermissionStatusInterface arg1, java.lang.String arg2, java.lang.String arg3, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag> arg4, boolean arg5, short arg6, long arg7, org.apache.hadoop.crypto.CryptoProtocolVersion[] arg8, java.lang.String arg9, java.lang.String arg10, boolean arg11) throws java.io.IOException;
    int getCorruptFilesCount();
    java.util.List<java.net.URI> getNamespaceEditsDirs(ConfigurationInterface arg0) throws java.io.IOException;
    boolean isImageLoaded();
    java.util.List<org.apache.hadoop.fs.XAttr> getXAttrs(java.lang.String arg0, java.util.List<org.apache.hadoop.fs.XAttr> arg1) throws java.io.IOException;
    void renewLease(java.lang.String arg0) throws java.io.IOException;
    long nextBlockId(org.apache.hadoop.hdfs.protocol.BlockType arg0) throws java.io.IOException;
    boolean getCallerContextEnabled();
    void handleLifeline(DatanodeRegistrationInterface arg0, org.apache.hadoop.hdfs.server.protocol.StorageReport[] arg1, long arg2, long arg3, int arg4, int arg5, int arg6, VolumeFailureSummaryInterface arg7) throws java.io.IOException;
    void addCacheEntryWithPayload(byte[] arg0, int arg1, java.lang.Object arg2);
    long getProvidedCapacity();
    void setNNResourceChecker(NameNodeResourceCheckerInterface arg0);
    java.lang.String getSafemode();
    void clear();
    java.util.concurrent.locks.ReentrantReadWriteLock getFsLockForTests();
    void renameSnapshot(java.lang.String arg0, java.lang.String arg1, java.lang.String arg2, boolean arg3) throws java.io.IOException;
    java.lang.String getJournalTransactionInfo();
    long getLeaseRecheckIntervalMs();
    java.lang.String getNodeUsage();
    boolean isNeedRollbackFsImage();
    long getMaxLockHoldToReleaseLeaseMs();
    BlocksWithLocationsInterface getBlocks(DatanodeIDInterface arg0, long arg1, long arg2) throws java.io.IOException;
    void validateStoragePolicySatisfy() throws org.apache.hadoop.hdfs.server.namenode.UnsupportedActionException, java.io.IOException;
    void setFsLockForTests(java.util.concurrent.locks.ReentrantReadWriteLock arg0);
    java.lang.String getQuotaCommand(long arg0, long arg1);
    void setPermission(java.lang.String arg0, FsPermissionInterface arg1) throws java.io.IOException;
    SafeModeExceptionInterface newSafemodeException(java.lang.String arg0);
    boolean truncate(java.lang.String arg0, long arg1, java.lang.String arg2, java.lang.String arg3, long arg4) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;
    void enableAsyncAuditLog(ConfigurationInterface arg0);
    void cpUnlock();
    void fsync(java.lang.String arg0, long arg1, java.lang.String arg2, long arg3) throws java.io.IOException;
    long getNNStartedTimeInMillis();
    int getReadHoldCount();
    boolean restoreFailedStorage(java.lang.String arg0) throws java.io.IOException;
    boolean enableErasureCodingPolicy(java.lang.String arg0, boolean arg1) throws java.io.IOException;
    void removeXattr(long arg0, java.lang.String arg1) throws java.io.IOException;
    void startCommonServices(ConfigurationInterface arg0, org.apache.hadoop.hdfs.server.namenode.ha.HAContext arg1) throws java.io.IOException;
    void logAuditEvent(boolean arg0, java.lang.String arg1, java.lang.String arg2, org.apache.hadoop.hdfs.protocol.HdfsFileStatus arg3) throws java.io.IOException;
    RollingUpgradeInfoInterface finalizeRollingUpgrade() throws java.io.IOException;
    long getUnderReplicatedBlocks();
    TokenInterface<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier> getDelegationToken(TextInterface arg0) throws java.io.IOException;
    org.apache.hadoop.hdfs.protocol.HdfsFileStatus startFileInt(java.lang.String arg0, PermissionStatusInterface arg1, java.lang.String arg2, java.lang.String arg3, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag> arg4, boolean arg5, short arg6, long arg7, org.apache.hadoop.crypto.CryptoProtocolVersion[] arg8, java.lang.String arg9, java.lang.String arg10, boolean arg11) throws java.io.IOException;
    RetryCacheInterface getRetryCache();
    RetryCacheInterface initRetryCache(ConfigurationInterface arg0);
    void addCommittedBlocksToPending(INodeFileInterface arg0);
    void writeUnlock(java.lang.String arg0);
    void reportBadBlocks(LocatedBlockInterface[] arg0) throws java.io.IOException;
    int getNumLiveDataNodes();
    void setQuota(java.lang.String arg0, long arg1, long arg2, org.apache.hadoop.fs.StorageType arg3) throws java.io.IOException;
    void releaseBackupNode(NamenodeRegistrationInterface arg0) throws java.io.IOException;
    void refreshNodes() throws java.io.IOException;
    long getNumFilesUnderConstruction();
    long getTotalSyncCount();
    long getPendingDeletionECBlocks();
    java.lang.String getDecomNodes();
    int getTotalLoad();
    long getUsed();
    void removeCachePool(java.lang.String arg0, boolean arg1) throws java.io.IOException;
    LeaseInterface reassignLeaseInternal(LeaseInterface arg0, java.lang.String arg1, INodeFileInterface arg2);
    long getLastCheckpointTime();
    void readLockInterruptibly() throws java.lang.InterruptedException;
    int getBlockCapacity();
    long getBlockPoolUsedSpace();
    void checkRollingUpgrade(java.lang.String arg0) throws org.apache.hadoop.hdfs.protocol.RollingUpgradeException;
    BatchedListEntriesInterface<org.apache.hadoop.hdfs.protocol.CachePoolEntry> listCachePools(java.lang.String arg0) throws java.io.IOException;
    long getLastWrittenTransactionId();
    void triggerRollbackCheckpoint();
    void registerMBean();
    void readUnlock();
    void checkSuperuserPrivilege(java.lang.String arg0) throws java.io.IOException;
    boolean saveNamespace(long arg0, long arg1) throws java.io.IOException;
    int getNumStaleDataNodes();
    int getDistinctVersionCount();
    void logAuditEvent(boolean arg0, java.lang.String arg1, java.lang.String arg2, java.lang.String arg3, FileStatusInterface arg4) throws java.io.IOException;
    CacheManagerInterface getCacheManager();
    INodeFileInterface checkUCBlock(ExtendedBlockInterface arg0, java.lang.String arg1) throws java.io.IOException;
    org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod getConnectionAuthenticationMethod() throws java.io.IOException;
    long getFilesTotal();
    void stopSecretManager();
    SnapshotDiffReportListingInterface getSnapshotDiffReportListing(java.lang.String arg0, java.lang.String arg1, java.lang.String arg2, byte[] arg3, int arg4) throws java.io.IOException;
    void cpLock();
    void deleteSnapshot(java.lang.String arg0, java.lang.String arg1, boolean arg2) throws java.io.IOException;
    BlockStoragePolicyInterface[] getStoragePolicies() throws java.io.IOException;
    void addCachePool(CachePoolInfoInterface arg0, boolean arg1) throws java.io.IOException;
    LeaseManagerInterface getLeaseManager();
    void unsetStoragePolicy(java.lang.String arg0) throws java.io.IOException;
    int getNumSnapshots();
    BlockManagerInterface getBlockManager();
    void commitOrCompleteLastBlock(INodeFileInterface arg0, INodesInPathInterface arg1, BlockInterface arg2) throws java.io.IOException;
    boolean nameNodeHasResourcesAvailable();
    FSDirectoryInterface getFSDirectory();
    void commitBlockSynchronization(ExtendedBlockInterface arg0, long arg1, long arg2, boolean arg3, boolean arg4, DatanodeIDInterface[] arg5, java.lang.String[] arg6) throws java.io.IOException;
    long getCTime();
    BatchedListEntriesInterface<org.apache.hadoop.hdfs.protocol.EncryptionZone> listEncryptionZones(long arg0) throws java.io.IOException;
    void endCheckpoint(NamenodeRegistrationInterface arg0, CheckpointSignatureInterface arg1) throws java.io.IOException;
    void abandonBlock(ExtendedBlockInterface arg0, long arg1, java.lang.String arg2, java.lang.String arg3) throws java.io.IOException;
    DelegationTokenSecretManagerInterface getDelegationTokenSecretManager();
    int getNumDecomLiveDataNodes();
    HeartbeatResponseInterface handleHeartbeat(DatanodeRegistrationInterface arg0, org.apache.hadoop.hdfs.server.protocol.StorageReport[] arg1, long arg2, long arg3, int arg4, int arg5, int arg6, VolumeFailureSummaryInterface arg7, boolean arg8, SlowPeerReportsInterface arg9, SlowDiskReportsInterface arg10) throws java.io.IOException;
    boolean testRMIPrint(java.lang.String arg0);
    void removeDefaultAcl(java.lang.String arg0) throws java.io.IOException;
    void checkAvailableResources();
    ErasureCodingPolicyInfoInterface[] getErasureCodingPolicies() throws java.io.IOException;
    long getNonDfsUsedSpace();
    boolean internalReleaseLease(LeaseInterface arg0, java.lang.String arg1, INodesInPathInterface arg2, java.lang.String arg3) throws java.io.IOException;
    void disallowSnapshot(java.lang.String arg0) throws java.io.IOException;
    void unsetErasureCodingPolicy(java.lang.String arg0, boolean arg1) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.hdfs.server.namenode.SafeModeException, org.apache.hadoop.security.AccessControlException;
    long getDfsUsed(DatanodeDescriptorInterface arg0);
    long getPendingDeletionReplicatedBlocks();
    BatchedListEntriesInterface<org.apache.hadoop.hdfs.protocol.OpenFileEntry> listOpenFiles(long arg0, java.util.EnumSet<org.apache.hadoop.hdfs.protocol.OpenFilesIterator.OpenFilesType> arg1, java.lang.String arg2) throws java.io.IOException;
    boolean delete(java.lang.String arg0, boolean arg1, boolean arg2) throws java.io.IOException;
    boolean disableErasureCodingPolicy(java.lang.String arg0, boolean arg1) throws java.io.IOException;
    boolean recoverLease(java.lang.String arg0, java.lang.String arg1, java.lang.String arg2) throws java.io.IOException;
    ECTopologyVerifierResultInterface getEcTopologyVerifierResultForEnabledPolicies();
    void removeAclEntries(java.lang.String arg0, java.util.List<org.apache.hadoop.fs.permission.AclEntry> arg1) throws java.io.IOException;
    int getFsLockQueueLength();
    java.lang.String getNameDirStatuses();
    int getMaxListOpenFilesResponses();
    void writeLock();
    java.util.List<org.apache.hadoop.fs.XAttr> listXAttrs(java.lang.String arg0) throws java.io.IOException;
    BatchedDirectoryListingInterface getBatchedListing(java.lang.String[] arg0, byte[] arg1, boolean arg2) throws java.io.IOException;
    NamespaceInfoInterface unprotectedGetNamespaceInfo();
    DatanodeStorageReportInterface[] getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType arg0) throws java.io.IOException;
    long getProvidedCapacityTotal();
    void setTimes(java.lang.String arg0, long arg1, long arg2) throws java.io.IOException;
    void removeCacheDirective(long arg0, boolean arg1) throws java.io.IOException;
    int getNumInMaintenanceLiveDataNodes();
    boolean isFileDeleted(INodeFileInterface arg0);
    DatanodeInfoInterface[] slowDataNodesReport() throws java.io.IOException;
    void close();
    void leaveSafeMode(boolean arg0);
    java.lang.String getClusterId();
    boolean hasReadLock();
    boolean isInSnapshot(long arg0);
    void setBalancerBandwidth(long arg0) throws java.io.IOException;
    long getMissingBlocksCount();
    void loadFSImage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption arg0) throws java.io.IOException;
    java.util.Collection<java.net.URI> getNamespaceDirs(ConfigurationInterface arg0);
    long getLowRedundancyBlocks();
    void setFSDirectory(FSDirectoryInterface arg0);
    long getBlocksTotal();
    void registerBackupNode(NamenodeRegistrationInterface arg0, NamenodeRegistrationInterface arg1) throws java.io.IOException;
    int getNumStaleStorages();
    long getCompleteBlocksTotal();
//    java.util.Collection<org.apache.hadoop.hdfs.server.namenode.FSNamesystem.CorruptFileBlockInfo> listCorruptFileBlocks(java.lang.String arg0, java.lang.String[] arg1) throws java.io.IOException;
    void registerMXBean();
    void modifyAclEntries(java.lang.String arg0, java.util.List<org.apache.hadoop.fs.permission.AclEntry> arg1) throws java.io.IOException;
    LocatedBlockInterface getAdditionalBlock(java.lang.String arg0, long arg1, java.lang.String arg2, ExtendedBlockInterface arg3, DatanodeInfoInterface[] arg4, java.lang.String[] arg5, java.util.EnumSet<org.apache.hadoop.hdfs.AddBlockFlag> arg6) throws java.io.IOException;
    void shutdown();
    ErasureCodingPolicyManagerInterface getErasureCodingPolicyManager();
    void allowSnapshot(java.lang.String arg0) throws java.io.IOException;
    void readUnlock(java.lang.String arg0);
    java.util.Collection<java.net.URI> getRequiredNamespaceEditsDirs(ConfigurationInterface arg0);
    long getNumOfReadLockLongHold();
    void checkOperation(org.apache.hadoop.hdfs.server.namenode.NameNode.OperationCategory arg0) throws org.apache.hadoop.ipc.StandbyException;
    void renameTo(java.lang.String arg0, java.lang.String arg1, boolean arg2, org.apache.hadoop.fs.Options.Rename... arg3) throws java.io.IOException;
    float getPercentRemaining();
    void logUpdateMasterKey(DelegationKeyInterface arg0);
//    INodeFileInterface getBlockCollection(long arg0);
    int getPendingDataNodeMessageCount();
    long getHighestPriorityLowRedundancyECBlocks();
    long getTransactionsSinceLastCheckpoint();
    long getCorrectTransactionsSinceLastLogRoll();
    long getMillisSinceLastLoadedEdits();
    long getMaxObjects();
    long getNumOfWriteLockLongHold();
    void checkConfiguration(ConfigurationInterface arg0) throws java.io.IOException;
//    boolean recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem.RecoverLeaseOp arg0, INodesInPathInterface arg1, java.lang.String arg2, java.lang.String arg3, java.lang.String arg4, boolean arg5) throws java.io.IOException;
    java.util.List<org.apache.hadoop.hdfs.server.namenode.AuditLogger> getAuditLoggers();
    boolean isObserver();
    java.lang.String getLiveNodes();
    ContentSummaryInterface getContentSummary(java.lang.String arg0) throws java.io.IOException;
    BatchedListEntriesInterface<org.apache.hadoop.hdfs.protocol.CacheDirectiveEntry> listCacheDirectives(long arg0, CacheDirectiveInfoInterface arg1) throws java.io.IOException;
    RollingUpgradeInfoInterface getRollingUpgradeInfo();
    void setCallerContextEnabled(boolean arg0);
    void saveSecretManagerStateCompat(java.io.DataOutputStream arg0, java.lang.String arg1) throws java.io.IOException;
    void prepareToStopStandbyServices() throws org.apache.hadoop.ha.ServiceFailedException;
    SnapshotManagerInterface getSnapshotManager();
    boolean inActiveState();
    java.util.List<java.lang.String> listCorruptFileBlocksWithSnapshot(java.lang.String arg0, java.util.List<java.lang.String> arg1, java.lang.String[] arg2) throws java.io.IOException;
    long getBytesInFuture();
    boolean isAuditEnabled();
    void cancelDelegationToken(TokenInterface<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier> arg0) throws java.io.IOException;
    boolean completeFile(java.lang.String arg0, java.lang.String arg1, ExtendedBlockInterface arg2, long arg3) throws java.io.IOException;
    java.util.List<java.net.URI> getSharedEditsDirs(ConfigurationInterface arg0);
    AddErasureCodingPolicyResponseInterface[] addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[] arg0, boolean arg1) throws java.io.IOException;
    long getTotalReplicatedBlocks();
    void checkBlockLocationsWhenObserver(LocatedBlocksInterface arg0, java.lang.String arg1) throws org.apache.hadoop.ipc.ObserverRetryOnActiveException;
    long getLastContact(DatanodeDescriptorInterface arg0);
    java.util.Map<java.lang.String, java.lang.Integer> getDistinctVersions();
    java.util.Date getStartTime();
    LocatedBlocksInterface getBlockLocations(java.lang.String arg0, java.lang.String arg1, long arg2, long arg3) throws java.io.IOException;
    EditLogTailerInterface getEditLogTailer();
    float getPercentBlockPoolUsed();
    void checkErasureCodingSupported(java.lang.String arg0) throws org.apache.hadoop.hdfs.server.namenode.UnsupportedActionException;
    long getNumberOfMissingBlocksWithReplicationFactorOne();
    long getCapacityRemaining();
    long getNumActiveClients();
    java.util.List<java.net.URI> getNamespaceEditsDirs(ConfigurationInterface arg0, boolean arg1) throws java.io.IOException;
    void imageLoadComplete();
    BatchedListEntriesInterface<org.apache.hadoop.hdfs.protocol.OpenFileEntry> getFilesBlockingDecom(long arg0, java.lang.String arg1);
    long getNumberOfMissingBlocks();
    java.lang.String getVerifyECWithTopologyResult();
    long getMissingECBlockGroups();
    boolean hasWriteLock();
    LeaseInterface reassignLease(LeaseInterface arg0, java.lang.String arg1, java.lang.String arg2, INodeFileInterface arg3);
    DirectoryListingInterface getListing(java.lang.String arg0, byte[] arg1, boolean arg2) throws java.io.IOException;
    long getLowRedundancyECBlockGroups();
    org.apache.hadoop.hdfs.server.namenode.ha.HAContext getHAContext();
    java.lang.String getRegistrationID();
    void updatePipeline(java.lang.String arg0, ExtendedBlockInterface arg1, ExtendedBlockInterface arg2, DatanodeIDInterface[] arg3, java.lang.String[] arg4, boolean arg5) throws java.io.IOException;
    int getNumSnapshottableDirs();
    void stopActiveServices();
    long[] getStats();
    org.apache.hadoop.crypto.CryptoProtocolVersion chooseProtocolVersion(EncryptionZoneInterface arg0, org.apache.hadoop.crypto.CryptoProtocolVersion[] arg1) throws org.apache.hadoop.hdfs.UnknownCryptoProtocolVersionException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.hdfs.protocol.SnapshotAccessControlException;
    boolean isHaEnabled();
    java.util.Map<java.lang.String, java.lang.String> getErasureCodingCodecs() throws java.io.IOException;
    ErasureCodingPolicyInterface getErasureCodingPolicy(java.lang.String arg0) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;
    LocatedBlockInterface bumpBlockGenerationStamp(ExtendedBlockInterface arg0, java.lang.String arg1) throws java.io.IOException;
    java.lang.String getCompileInfo();
    long getMissingReplicationOneBlocks();
    int getNumDeadDataNodes();
    long getLazyPersistFileScrubberTS();
    long getCorruptECBlockGroups();
    java.lang.String leaseExceptionString(java.lang.String arg0, long arg1, java.lang.String arg2);
    void requireEffectiveLayoutVersionForFeature(org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion.Feature arg0) throws org.apache.hadoop.HadoopIllegalArgumentException;
    CheckpointSignatureInterface rollEditLog() throws java.io.IOException;
    void readLock();
    long getPendingDeletionBlocks();
    int getExpiredHeartbeats();
    java.lang.String getCorruptFiles();
    void unlockRetryCache();
    void logExpireDelegationToken(DelegationTokenIdentifierInterface arg0);
    long nextGenerationStamp(boolean arg0) throws java.io.IOException;
    java.lang.String getTopUserOpCounts();
    void checkNameNodeSafeMode(java.lang.String arg0) throws org.apache.hadoop.ipc.RetriableException, org.apache.hadoop.hdfs.server.namenode.SafeModeException;
    void lockRetryCache();
    UserGroupInformationInterface getRemoteUser() throws java.io.IOException;
    float getCapacityUsedGB();
    ReplicatedBlockStatsInterface getReplicatedBlockStats();
    PermissionStatusInterface createFsOwnerPermissions(FsPermissionInterface arg0);
    void registerDatanode(DatanodeRegistrationInterface arg0) throws java.io.IOException;
    java.lang.String getBlockPoolId();
    INodeFileInterface checkLease(INodesInPathInterface arg0, java.lang.String arg1, long arg2) throws org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException, java.io.FileNotFoundException;
    long getHighestPriorityLowRedundancyReplicatedBlocks();
    long getMissingReplOneBlocksCount();
    boolean setReplication(java.lang.String arg0, short arg1) throws java.io.IOException;
    FSImageInterface getFSImage();
    BlockInfoInterface getStoredBlock(BlockInterface arg0);
    void createSymlink(java.lang.String arg0, java.lang.String arg1, PermissionStatusInterface arg2, boolean arg3, boolean arg4) throws java.io.IOException;
    long getTransactionsSinceLastLogRoll();
    long getNumTimedOutPendingReconstructions();
}