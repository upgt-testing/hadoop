package org.apache.hadoop.hdfs.remoteProxies;

public interface DataStorageInterface {
    void clearRollingUpgradeMarker(java.lang.String arg0) throws java.io.IOException;
    java.lang.Iterable<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory> dirIterable(org.apache.hadoop.hdfs.server.common.Storage.StorageDirType arg0);
    int getNamespaceID();
    void linkAllBlocks(java.io.File arg0, java.io.File arg1, java.io.File arg2, int arg3, ConfigurationInterface arg4) throws java.io.IOException;
    void setServiceLayoutVersion(int arg0);
    int getServiceLayoutVersion();
//    java.util.ArrayList<org.apache.hadoop.hdfs.server.datanode.DataStorage.LinkArgs> findDuplicateEntries(java.util.ArrayList<org.apache.hadoop.hdfs.server.datanode.DataStorage.LinkArgs> arg0);
    void doUpgradePreFederation(StorageDirectoryInterface arg0, NamespaceInfoInterface arg1, java.util.List<java.util.concurrent.Callable<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory>> arg2, ConfigurationInterface arg3) throws java.io.IOException;
    java.util.List<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory> addStorageLocations(DataNodeInterface arg0, NamespaceInfoInterface arg1, java.util.Collection<org.apache.hadoop.hdfs.server.datanode.StorageLocation> arg2, org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption arg3) throws java.io.IOException;
    void setFieldsFromProperties(java.util.Properties arg0, StorageDirectoryInterface arg1, boolean arg2, int arg3) throws java.io.IOException;
    boolean confirmFormat(java.lang.Iterable<? extends org.apache.hadoop.hdfs.server.common.Storage.FormatConfirmable> arg0, boolean arg1, boolean arg2) throws java.io.IOException;
    java.util.List<java.io.File> getFiles(org.apache.hadoop.hdfs.server.common.Storage.StorageDirType arg0, java.lang.String arg1);
    java.util.Map<java.lang.Integer, java.util.SortedSet<org.apache.hadoop.hdfs.protocol.LayoutVersion.LayoutFeature>> getServiceLayoutFeatureMap();
    java.lang.String listStorageDirectories();
    void readProperties(StorageDirectoryInterface arg0) throws java.io.IOException;
    void addStorageDir(StorageDirectoryInterface arg0);
    void format(StorageDirectoryInterface arg0, NamespaceInfoInterface arg1, java.lang.String arg2, ConfigurationInterface arg3) throws java.io.IOException;
    BlockPoolSliceStorageInterface getBPStorage(java.lang.String arg0);
    void rename(java.io.File arg0, java.io.File arg1) throws java.io.IOException;
    void setcTime(java.util.Properties arg0, StorageDirectoryInterface arg1) throws org.apache.hadoop.hdfs.server.common.InconsistentFSStateException;
    void writeProperties(StorageDirectoryInterface arg0) throws java.io.IOException;
    java.lang.String toColonSeparatedString();
    int getNsIdFromColonSeparatedString(java.lang.String arg0);
    void unlockAll() throws java.io.IOException;
    StorageDirectoryInterface loadStorageDirectory(DataNodeInterface arg0, NamespaceInfoInterface arg1, StorageLocationInterface arg2, org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption arg3, java.util.List<java.util.concurrent.Callable<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory>> arg4) throws java.io.IOException;
    long getCTime();
    boolean fullyDelete(java.io.File arg0);
    void doRollback(StorageDirectoryInterface arg0, NamespaceInfoInterface arg1) throws java.io.IOException;
    java.lang.String getRegistrationID(StorageInfoInterface arg0);
    void removeBlockPoolStorage(java.lang.String arg0);
    boolean doTransition(StorageDirectoryInterface arg0, NamespaceInfoInterface arg1, org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption arg2, java.util.List<java.util.concurrent.Callable<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory>> arg3, ConfigurationInterface arg4) throws java.io.IOException;
    void setDatanodeUuid(java.lang.String arg0);
    void setClusterId(java.util.Properties arg0, int arg1, StorageDirectoryInterface arg2) throws org.apache.hadoop.hdfs.server.common.InconsistentFSStateException;
    java.lang.String getClusterIdFromColonSeparatedString(java.lang.String arg0);
    java.lang.String getBuildVersion();
    int getLayoutVersion();
    java.util.Properties readPropertiesFile(java.io.File arg0) throws java.io.IOException;
    void readPreviousVersionProperties(StorageDirectoryInterface arg0) throws java.io.IOException;
    java.util.Iterator<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory> dirIterator(boolean arg0);
    void writeAll() throws java.io.IOException;
    java.lang.String getClusterID();
    java.util.List<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory> loadBlockPoolSliceStorage(DataNodeInterface arg0, NamespaceInfoInterface arg1, java.util.Collection<org.apache.hadoop.hdfs.server.datanode.StorageLocation> arg2, org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption arg3, java.util.concurrent.ExecutorService arg4) throws java.io.IOException;
    int getNumStorageDirs();
    VolumeBuilderInterface prepareVolume(DataNodeInterface arg0, StorageLocationInterface arg1, java.util.List<org.apache.hadoop.hdfs.server.protocol.NamespaceInfo> arg2) throws java.io.IOException;
    void linkBlocks(java.io.File arg0, java.io.File arg1, java.lang.String arg2, int arg3, HardLinkInterface arg4, ConfigurationInterface arg5) throws java.io.IOException;
    boolean createStorageID(StorageDirectoryInterface arg0, int arg1, ConfigurationInterface arg2);
//    void removeDuplicateEntries(java.util.ArrayList<org.apache.hadoop.hdfs.server.datanode.DataStorage.LinkArgs> arg0, java.util.ArrayList<org.apache.hadoop.hdfs.server.datanode.DataStorage.LinkArgs> arg1);
    void setFieldsFromProperties(java.util.Properties arg0, StorageDirectoryInterface arg1) throws java.io.IOException;
    void doUpgrade(StorageDirectoryInterface arg0, NamespaceInfoInterface arg1, java.io.File arg2, java.io.File arg3, java.io.File arg4, java.io.File arg5, int arg6, ConfigurationInterface arg7) throws java.io.IOException;
    java.lang.String toString();
    void upgradeProperties(StorageDirectoryInterface arg0, ConfigurationInterface arg1) throws java.io.IOException;
    boolean createStorageID(StorageDirectoryInterface arg0, boolean arg1, ConfigurationInterface arg2);
    boolean trashEnabled(java.lang.String arg0);
    boolean containsStorageDir(java.io.File arg0) throws java.io.IOException;
    NamespaceInfoInterface getNamespaceInfo();
    void writeProperties(java.io.File arg0, java.util.Properties arg1) throws java.io.IOException;
    java.lang.String getTrashDirectoryForReplica(java.lang.String arg0, ReplicaInfoInterface arg1);
    void writeProperties(java.io.File arg0, StorageDirectoryInterface arg1) throws java.io.IOException;
    void doFinalize(StorageDirectoryInterface arg0) throws java.io.IOException;
    java.util.Iterator<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory> dirIterator(org.apache.hadoop.hdfs.server.common.Storage.StorageDirType arg0, boolean arg1);
    boolean isPreUpgradableLayout(StorageDirectoryInterface arg0) throws java.io.IOException;
    java.lang.String getDatanodeUuid();
    java.util.Iterator<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory> dirIterator();
    void setStorageInfo(StorageInfoInterface arg0);
    void setPropertiesFromFields(java.util.Properties arg0, StorageDirectoryInterface arg1) throws java.io.IOException;
    void linkBlocks(java.io.File arg0, java.io.File arg1, int arg2, HardLinkInterface arg3, ConfigurationInterface arg4) throws java.io.IOException;
    boolean containsStorageDir(StorageLocationInterface arg0, java.lang.String arg1) throws java.io.IOException;
    void setLayoutVersion(java.util.Properties arg0, StorageDirectoryInterface arg1) throws org.apache.hadoop.hdfs.server.common.IncorrectVersionException, org.apache.hadoop.hdfs.server.common.InconsistentFSStateException;
    boolean versionSupportsFederation(java.util.Map<java.lang.Integer, java.util.SortedSet<org.apache.hadoop.hdfs.protocol.LayoutVersion.LayoutFeature>> arg0);
    void setRollingUpgradeMarker(java.lang.String arg0) throws java.io.IOException;
    java.util.List<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory> getStorageDirs();
    void setNamespaceID(java.util.Properties arg0, StorageDirectoryInterface arg1) throws org.apache.hadoop.hdfs.server.common.InconsistentFSStateException;
    void checkVersionUpgradable(int arg0) throws java.io.IOException;
    BlockPoolSliceStorageInterface getBlockPoolSliceStorage(NamespaceInfoInterface arg0);
    void clearTrash(java.lang.String arg0);
    void nativeCopyFileUnbuffered(java.io.File arg0, java.io.File arg1, boolean arg2) throws java.io.IOException;
    void enableTrash(java.lang.String arg0);
    boolean containsStorageDir(StorageLocationInterface arg0) throws java.io.IOException;
    boolean is203LayoutVersion(int arg0);
    java.util.Iterator<org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory> dirIterator(org.apache.hadoop.hdfs.server.common.Storage.StorageDirType arg0);
    java.lang.String getProperty(java.util.Properties arg0, StorageDirectoryInterface arg1, java.lang.String arg2) throws org.apache.hadoop.hdfs.server.common.InconsistentFSStateException;
    void checkStorageType(java.util.Properties arg0, StorageDirectoryInterface arg1) throws org.apache.hadoop.hdfs.server.common.InconsistentFSStateException;
    void cleanupDetachDir(java.io.File arg0) throws java.io.IOException;
    void finalizeUpgrade(java.lang.String arg0) throws java.io.IOException;
    void checkOldLayoutStorage(StorageDirectoryInterface arg0) throws java.io.IOException;
    void removeVolumes(java.util.Collection<org.apache.hadoop.hdfs.server.datanode.StorageLocation> arg0) throws java.io.IOException;
    void readProperties(StorageDirectoryInterface arg0, int arg1) throws java.io.IOException;
    int getParallelVolumeLoadThreadsNum(int arg0, ConfigurationInterface arg1);
    StorageDirectoryInterface getStorageDir(int arg0);
    void deleteDir(java.io.File arg0) throws java.io.IOException;
    void recoverTransitionRead(DataNodeInterface arg0, NamespaceInfoInterface arg1, java.util.Collection<org.apache.hadoop.hdfs.server.datanode.StorageLocation> arg2, org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption arg3) throws java.io.IOException;
    java.util.List<org.apache.hadoop.hdfs.server.datanode.StorageLocation> loadDataStorage(DataNodeInterface arg0, NamespaceInfoInterface arg1, java.util.Collection<org.apache.hadoop.hdfs.server.datanode.StorageLocation> arg2, org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption arg3, java.util.concurrent.ExecutorService arg4) throws java.io.IOException;
//    void linkBlocksHelper(java.io.File arg0, java.io.File arg1, HardLinkInterface arg2, boolean arg3, java.io.File arg4, java.util.List<org.apache.hadoop.hdfs.server.datanode.DataStorage.LinkArgs> arg5, java.util.Map<java.io.File, java.io.File> arg6) throws java.io.IOException;
    StorageDirectoryInterface getSingularStorageDir();
}